---
layout: post
title: 深度学习基本知识
subtitle:
date: 2021-10-01
author: ZhuSheng
header-img: img/the-first.png
catalog: true
tags:
- 机器学习
- 深度学习
---
# 深度学习基本知识

下面的应用基于 Tensorflow-Playground 实现
网址：http://playground.tensorflow.org

## 前言

### _比较好的博文推荐_

1. https://www.jianshu.com/p/ec41ff0e5943 通俗易懂介绍深度学习基本知识

## 1 基本的数据分布类型

圆环型：非线性
<img src="img/circle.png" alt="avatar" style="zoom:40%;" />
异或型：非线性
<img src="img/eor.png" alt="avatar" style="zoom:40%" />
高斯型：线性可分
<img src="img/liner.png" alt="avatar" style="zoom:40%;" />
螺旋形：非线性
<img src="img/spiral.png" alt="avatar" style="zoom:40%;" />

## 2 神经网络模型的结构

**机器学习在本质就是寻找一个好用的函数。而人工神经网络最“牛逼”的地方在于，它可以在理论上证明：只需一个包含足够多神经元的隐藏层，多层前馈网络能以任意精度逼近任意复杂度的连续函数。**
<img src="img/model.png" alt="avatar" style="zoom:50%;" />
**如何计算神经网络的层数**
输入层不涉及计算，神经网络的层数由隐含层和输出层确定，上图是三层。
每一个层中可以包含多个神经单元。
输入层的 x1，x2 不是代表样本，而是一个样本的两个特征。

### 2.1 神经网络的分层

#### 2.1.1 输入层

和单级网络一样，该层只起到输入信号的扇出作用.所以在计算网络的层数时不被记入。该层负责接收来自网络外部的信息，被记作第 0 层。

#### 2.1.2 隐含层

**为什么需要隐含层？**
一句话：利用隐藏层来生成非线性特征，从而解决非线性问题，比如异或数据类型。面对异或问题我们可以用 x1x2 来作为输入处理，但是很多情况下我们不知道我们数据的分布的情况下，且只有线性的输入时。这个时候隐含层就用于模拟 x1x2 的特征工程。例如不含隐含层，只用 x1、x2 分类异或数据，和在这个基础上添加两层隐含层后处理异或数据。

**隐藏层越多越好吗？**
一般而言，隐含层越多，隐含层神经元个数越多，越能发现底层的非线性规律，但同时也会导致深度过深，越难训练，训练时间越长。

#### 2.1.3 输出层

神经网络的最后一层，主要负责输出网络的计算结果

### 2.2 神经网络的传播
数据在网络中的传播有两种方式。一种是沿着输入到输出的路径，被称为前向传播。一种是从输出返回到输入，被成为反向传播(backprop)。

#### 2.2.1 前向传播（Forward Propagation）
前向传播就是从输入层，经过一层层的隐含层，不断计算每一层的中间变量(z)和激活值(a)，最后得到输出 y^ 的过程，计算出了 y^，就可以根据它和真实值 y 的差别来计算损失（loss）。

#### 2.2.2 反向传播（Backward Propagation）
反向传播就是根据损失函数 L(y^,y)来反方向地计算每一层的 z、a、w、b 的偏导数（梯度），从而更新参数。

**小结**

- 每经过一次前向传播和反向传播之后，参数就更新一次，然后用新的参数再次循环上面的过程。这就是神经网络训练的整个过程。
- 正向传播沿着从输入层到输出层的顺序，依次计算并存储神经网络的中间变量。
- 反向传播沿着从输出层到输入层的顺序，依次计算并存储神经网络中间变量和参数的梯度。
- 在训练深度学习模型时，正向传播和反向传播相互依赖。正向传播的计算可能依赖于模型参数的当前值，而这些模型参数是在反向传播的梯度计算后通过优化算法迭代的。反向传播的梯度计算可能依赖于各变量的当前值，而这些变量的当前值是通过正向传播计算得到的。

### 2.2 神经网络的类别
常用的神经网络包含了：DNN，CNN，RNN
#### 2.2.1 DNN

#### 2.2.2 CNN

#### 2.2.3 RNN


# 术语
ANN，Artificial Neural Network，人工神经网络

